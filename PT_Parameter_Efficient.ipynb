{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PT_Parameter-Efficient.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "HimA3TqEFSa3",
        "4svLK7RuPx8g"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fb522d77b2a242ebaa0d1c229301c2ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_953ee1e67fc44ad08d9e4aac65be7f6e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_611b2e6776934b6f9a7a4171f39035a8",
              "IPY_MODEL_a176f20f924b47fb8e47365b47ea9ba7"
            ]
          }
        },
        "953ee1e67fc44ad08d9e4aac65be7f6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "611b2e6776934b6f9a7a4171f39035a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_797eb394179243589d819b2d901bd094",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e028cbdc8c2d46de9a7051df965bc357"
          }
        },
        "a176f20f924b47fb8e47365b47ea9ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_96abf4dd02ab4b219f06a185d1ed7b66",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:02&lt;00:00, 150B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7fbc90fbd3d942ab948ab7b827e93118"
          }
        },
        "797eb394179243589d819b2d901bd094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e028cbdc8c2d46de9a7051df965bc357": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "96abf4dd02ab4b219f06a185d1ed7b66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7fbc90fbd3d942ab948ab7b827e93118": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6816bd47755648429147c899edd19f95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_be56d38b385048ee96c829da97a4d2a3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_379a41f18a9646ada57610f74f78342e",
              "IPY_MODEL_e99bdefb95c0429d965884d6fd0fb70d"
            ]
          }
        },
        "be56d38b385048ee96c829da97a4d2a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "379a41f18a9646ada57610f74f78342e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_66fafa9321f548c989669d9da8fc03ac",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a859baf1caea445590b72b17394d1738"
          }
        },
        "e99bdefb95c0429d965884d6fd0fb70d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bc6bd261434e4e11822e9d4f9c94777f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 254kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a562a936ed6b4d58aebfff7427885ce5"
          }
        },
        "66fafa9321f548c989669d9da8fc03ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a859baf1caea445590b72b17394d1738": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bc6bd261434e4e11822e9d4f9c94777f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a562a936ed6b4d58aebfff7427885ce5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "87a283a318874f83b637a58cf9bd7d6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5535a4615219465abfdb39e5b330ebd3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4f165300e51c44f9abe8dbac69de3b0e",
              "IPY_MODEL_351daf480fcc4085bf60e5d9bb7cc5da"
            ]
          }
        },
        "5535a4615219465abfdb39e5b330ebd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4f165300e51c44f9abe8dbac69de3b0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_152dab38d5b040a3a85476962bd85370",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e67405cc3fe948558dce8ccf155e2f59"
          }
        },
        "351daf480fcc4085bf60e5d9bb7cc5da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1b43c171934d49ec909d0dbfb87df994",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:06&lt;00:00, 68.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d4f2e10bbddc4f35b809a0a503ad5848"
          }
        },
        "152dab38d5b040a3a85476962bd85370": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e67405cc3fe948558dce8ccf155e2f59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1b43c171934d49ec909d0dbfb87df994": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d4f2e10bbddc4f35b809a0a503ad5848": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "spDQRw66hgT5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "e1835be3-54a4-4af9-f02d-ea30f75e4f95"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwSDwBaRnWup",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title Install Requirements\n",
        "from IPython.display import clear_output\n",
        "!pip install git+https://github.com/hosein-m/adapter-transformers.git@ML_Project2020\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NvVZ3WMRpjY",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "03d82347-7716-45f4-b1fd-0bd948bb098e"
      },
      "source": [
        "# @title Download GLUE + Additional datasets\n",
        "\"\"\" Script for downloading all GLUE data.\n",
        "Example usage:\n",
        "    python download_glue_data.py --data_dir data --tasks all\n",
        "Note: for legal reasons, we are unable to host MRPC.\n",
        "You can either use the version hosted by the SentEval team, which is already tokenized,\n",
        "or you can download the original data from:\n",
        "https://download.microsoft.com/download/D/4/6/D46FF87A-F6B9-4252-AA8B-3604ED519838/MSRParaphraseCorpus.msi  # noqa\n",
        "and extract the data from it manually.\n",
        "For Windows users, you can run the .msi file. For Mac and Linux users, consider an external library\n",
        "such as 'cabextract' (see below for an example). You should then rename and place specific files in\n",
        "a folder (see below for an example).\n",
        "mkdir MRPC\n",
        "cabextract MSRParaphraseCorpus.msi -d MRPC\n",
        "cat MRPC/_2DEC3DBE877E4DB192D17C0256E90F1D | tr -d $'\\r' > MRPC/msr_paraphrase_train.txt\n",
        "cat MRPC/_D7B391F9EAFF4B1B8BCE8F21B20B1B61 | tr -d $'\\r' > MRPC/msr_paraphrase_test.txt\n",
        "rm MRPC/_*\n",
        "rm MSRParaphraseCorpus.msi\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import tempfile\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "TASKS = [\"CoLA\", \"SST\", \"MRPC\", \"QQP\", \"STS\", \"MNLI\", \"QNLI\", \"RTE\", \"WNLI\",\"SPAM\",\"20NEWSGROUPS\",\"CRAIRLINE\",\"CRMESSAGE\", \"CRDISASTERS\",\"CRECONOMIC\",\"CREMOTION\",\"CRWARM\",\"CRPOLITICAL\",\n",
        "         \"CRBIAS\",\"CRPLTMSG\",\"CRPRMEMOTION\",\"CROPN\",\"NEWSAG\",\"CRUS\"]\n",
        "TASK2PATH = {\n",
        "    \"CoLA\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FCoLA.zip?alt=media&token=46d5e637-3411-4188-bc44-5809b5bfb5f4\",  # noqa\n",
        "    \"SST\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8\",  # noqa\n",
        "    \"MRPC\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc\",  # noqa\n",
        "    \"QQP\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQQP-clean.zip?alt=media&token=11a647cb-ecd3-49c9-9d31-79f8ca8fe277\",  # noqa\n",
        "    \"STS\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSTS-B.zip?alt=media&token=bddb94a7-8706-4e0d-a694-1109e12273b5\",  # noqa\n",
        "    \"MNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FMNLI.zip?alt=media&token=50329ea1-e339-40e2-809c-10c40afff3ce\",  # noqa\n",
        "    \"SNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSNLIv2.zip?alt=media&token=6c15d9df-a597-47d1-9a43-0c605cbc6ef9\",  # noqa\n",
        "    \"QNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQNLIv2.zip?alt=media&token=6fdcf570-0fc5-4631-8456-9505272d1601\",  # noqa\n",
        "    \"RTE\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb\",  # noqa\n",
        "    \"WNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FWNLI.zip?alt=media&token=068ad0a0-ded7-4bd7-99a5-5e00222e0faf\",  # noqa\n",
        "    \"SPAM\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\",\n",
        "    \"20NEWSGROUPS\": \"https://drive.google.com/uc?export=download&id=1KJkVWPwW_4GPhk2Oca8dedL9AE5cTVDL\",\n",
        "    \"CRAIRLINE\" : \"https://drive.google.com/uc?export=download&id=1YPwb9c3fWFgXDuq-zUU8hD2AJg090SZa\",\n",
        "    \"CRMESSAGE\" : \"https://drive.google.com/uc?export=download&id=1GiyOi-EdhSeq7uL92j1PXaEipE7quUq_\",\n",
        "    \"CRDISASTERS\": \"https://drive.google.com/uc?export=download&id=1EBo_TgSTFEaPQuPTOD4fVCvl3obCE0Dj\",\n",
        "    \"CRECONOMIC\" : \"https://drive.google.com/uc?export=download&id=1LSm9_WHPb1VicehKHDQbLydgqKhsQ0bR\",\n",
        "    \"CREMOTION\" : \"https://drive.google.com/uc?export=download&id=1UV0f5SIP_g_tw_Qfa2xtq2DSWhV0eDu-\",\n",
        "    \"CRWARM\" : \"https://drive.google.com/uc?export=download&id=1bPVIaVIojnzLZ8cX0Pr8322kG7fXeYgf\",\n",
        "    \"CRPOLITICAL\": \"https://drive.google.com/uc?export=download&id=1s3znAGBwtiM961Xrs2AWlE6EOl8_UcWp\",\n",
        "    \"CRBIAS\" : \"https://drive.google.com/uc?export=download&id=1-pv3hHU6-MBFgv0AE4pH1M1vhCuD_YQq\",\n",
        "    \"CRPLTMSG\":\"https://drive.google.com/uc?export=download&id=1OvZd1YNh0iod07DCW2Z1glEs4d7_2mmZ\",\n",
        "    \"CRPRMEMOTION\": \"https://drive.google.com/uc?export=download&id=1BoPiu4PidZABjDt2BPhCJsQyWE4EKKmP\",\n",
        "    \"CROPN\": \"https://drive.google.com/uc?export=download&id=1rJhCr0Bn56q-qbUEi637rTIsuX4b7qmu\",\n",
        "    \"NEWSAG\": \"https://drive.google.com/uc?export=download&id=17an27uSk7JVdJulq3p3FvR1oz9ty4m2g\",\n",
        "    \"CRUS\" : \"https://drive.google.com/uc?export=download&id=1ym1zO14CBYG7bGqBJStv2vbHkmtbNALp\",\n",
        "    \"diagnostic\": [\n",
        "        \"https://storage.googleapis.com/mtl-sentence-representations.appspot.com/tsvsWithoutLabels%2FAX.tsv?GoogleAccessId=firebase-adminsdk-0khhl@mtl-sentence-representations.iam.gserviceaccount.com&Expires=2498860800&Signature=DuQ2CSPt2Yfre0C%2BiISrVYrIFaZH1Lc7hBVZDD4ZyR7fZYOMNOUGpi8QxBmTNOrNPjR3z1cggo7WXFfrgECP6FBJSsURv8Ybrue8Ypt%2FTPxbuJ0Xc2FhDi%2BarnecCBFO77RSbfuz%2Bs95hRrYhTnByqu3U%2FYZPaj3tZt5QdfpH2IUROY8LiBXoXS46LE%2FgOQc%2FKN%2BA9SoscRDYsnxHfG0IjXGwHN%2Bf88q6hOmAxeNPx6moDulUF6XMUAaXCSFU%2BnRO2RDL9CapWxj%2BDl7syNyHhB7987hZ80B%2FwFkQ3MEs8auvt5XW1%2Bd4aCU7ytgM69r8JDCwibfhZxpaa4gd50QXQ%3D%3D\",  # noqa\n",
        "        \"https://www.dropbox.com/s/ju7d95ifb072q9f/diagnostic-full.tsv?dl=1\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "MRPC_TRAIN = \"https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\"\n",
        "MRPC_TEST = \"https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt\"\n",
        "\n",
        "\n",
        "def download_and_extract(task, data_dir):\n",
        "    print(\"Downloading and extracting %s...\" % task)\n",
        "    if task == \"SPAM\":\n",
        "      data_file = task\n",
        "      data_dir = data_dir + \"/\" + task\n",
        "    else :\n",
        "      data_file = \"%s.zip\" % task\n",
        "    urllib.request.urlretrieve(TASK2PATH[task], data_file)\n",
        "    with zipfile.ZipFile(data_file) as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "    os.remove(data_file)\n",
        "    print(\"\\tCompleted!\")\n",
        "\n",
        "\n",
        "def format_mrpc(data_dir, path_to_data):\n",
        "    print(\"Processing MRPC...\")\n",
        "    mrpc_dir = os.path.join(data_dir, \"MRPC\")\n",
        "    if not os.path.isdir(mrpc_dir):\n",
        "        os.mkdir(mrpc_dir)\n",
        "    if path_to_data:\n",
        "        mrpc_train_file = os.path.join(path_to_data, \"msr_paraphrase_train.txt\")\n",
        "        mrpc_test_file = os.path.join(path_to_data, \"msr_paraphrase_test.txt\")\n",
        "    else:\n",
        "        print(\"Local MRPC data not specified, downloading data from %s\" % MRPC_TRAIN)\n",
        "        mrpc_train_file = os.path.join(mrpc_dir, \"msr_paraphrase_train.txt\")\n",
        "        mrpc_test_file = os.path.join(mrpc_dir, \"msr_paraphrase_test.txt\")\n",
        "        urllib.request.urlretrieve(MRPC_TRAIN, mrpc_train_file)\n",
        "        urllib.request.urlretrieve(MRPC_TEST, mrpc_test_file)\n",
        "    assert os.path.isfile(mrpc_train_file), \"Train data not found at %s\" % mrpc_train_file\n",
        "    assert os.path.isfile(mrpc_test_file), \"Test data not found at %s\" % mrpc_test_file\n",
        "    urllib.request.urlretrieve(TASK2PATH[\"MRPC\"], os.path.join(mrpc_dir, \"dev_ids.tsv\"))\n",
        "\n",
        "    dev_ids = []\n",
        "    with open(os.path.join(mrpc_dir, \"dev_ids.tsv\"), encoding=\"utf8\") as ids_fh:\n",
        "        for row in ids_fh:\n",
        "            dev_ids.append(row.strip().split(\"\\t\"))\n",
        "\n",
        "    with open(mrpc_train_file, encoding=\"utf8\") as data_fh, open(\n",
        "        os.path.join(mrpc_dir, \"train.tsv\"), \"w\", encoding=\"utf8\"\n",
        "    ) as train_fh, open(os.path.join(mrpc_dir, \"dev.tsv\"), \"w\", encoding=\"utf8\") as dev_fh:\n",
        "        header = data_fh.readline()\n",
        "        train_fh.write(header)\n",
        "        dev_fh.write(header)\n",
        "        for row in data_fh:\n",
        "            label, id1, id2, s1, s2 = row.strip().split(\"\\t\")\n",
        "            if [id1, id2] in dev_ids:\n",
        "                dev_fh.write(\"%s\\t%s\\t%s\\t%s\\t%s\\n\" % (label, id1, id2, s1, s2))\n",
        "            else:\n",
        "                train_fh.write(\"%s\\t%s\\t%s\\t%s\\t%s\\n\" % (label, id1, id2, s1, s2))\n",
        "\n",
        "    with open(mrpc_test_file, encoding=\"utf8\") as data_fh, open(\n",
        "        os.path.join(mrpc_dir, \"test.tsv\"), \"w\", encoding=\"utf8\"\n",
        "    ) as test_fh:\n",
        "        header = data_fh.readline()\n",
        "        test_fh.write(\"index\\t#1 ID\\t#2 ID\\t#1 String\\t#2 String\\n\")\n",
        "        for idx, row in enumerate(data_fh):\n",
        "            label, id1, id2, s1, s2 = row.strip().split(\"\\t\")\n",
        "            test_fh.write(\"%d\\t%s\\t%s\\t%s\\t%s\\n\" % (idx, id1, id2, s1, s2))\n",
        "    print(\"\\tCompleted!\")\n",
        "\n",
        "\n",
        "def make_ready_example(task, spam_dir):\n",
        "  if task == 'SPAM':\n",
        "      df = pd.read_csv(spam_dir + '/SMSSpamCollection',sep = '\\t',header = None, encoding = 'latin-1')\n",
        "      df.columns = [\"label\", \"text\"]\n",
        "      df.label = pd.factorize(df.label)[0]\n",
        "      df.iloc[:4459].to_csv(spam_dir + '/train.tsv', sep='\\t')\n",
        "      df.iloc[4459:4459 + 557].to_csv(spam_dir + '/dev.tsv', sep='\\t')\n",
        "      df.iloc[4459 + 557:].to_csv(spam_dir + '/test.tsv', sep='\\t')\n",
        "  \n",
        "GLUE_DIR = \"glue_datasets\"\n",
        "if not os.path.isdir(GLUE_DIR):\n",
        "    os.mkdir(GLUE_DIR)\n",
        "\n",
        "spam_dir = os.path.join(GLUE_DIR, \"SPAM\")\n",
        "if not os.path.isdir(spam_dir):\n",
        "    os.mkdir(spam_dir)\n",
        "for task in TASKS:\n",
        "    if task == 'MRPC':\n",
        "        format_mrpc(GLUE_DIR, \"\")\n",
        "    else:\n",
        "        download_and_extract(task, GLUE_DIR)\n",
        "        if task == 'SPAM':\n",
        "          make_ready_example(task, spam_dir)\n",
        "clear_output()\n",
        "print(\"Completed\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-uVODxdpfW3",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6ce08765-7169-423c-ef47-6e24e5c82335"
      },
      "source": [
        "# @title Import Requirements\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.metrics import matthews_corrcoef, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import AutoModelWithHeads, AutoTokenizer, AdapterType\n",
        "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers.data.data_collator import DefaultDataCollator\n",
        "from transformers import (\n",
        "    glue_output_modes,\n",
        "    glue_tasks_num_labels,\n",
        "    glue_processors,\n",
        "    glue_convert_examples_to_features,\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "  device = torch.device(\"cuda\")\n",
        "  print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "  print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "  print('No GPU available, using the CPU instead.')\n",
        "  device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZrAcG7mr36i",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bbec8337-dae3-4ae5-a72f-db476342dc93"
      },
      "source": [
        "# @title Hyperparameters\n",
        "\n",
        "BATCH_SIZE =  32# @param {type:\"integer\"}\n",
        "REDUCTION_FACTOR = 12#@param {type:\"integer\"}\n",
        "NON_LINEARITY = \"gelu\" #@param [\"gelu\", \"relu\", \"sewish\"]\n",
        "EPOCHS =  10#@param {type:\"integer\"}\n",
        "MAX_LENGTH =   128#@param {type:\"integer\"}\n",
        "\n",
        "TASK = \"rte\" #@param [\"cola\", \"sst-2\", \"mrpc\", \"sts-b\", \"qqp\", \"mnli\", \"mnli-mm\", \"qnli\", \"rte\", \"wnli\",\"spam\",\"20newsgroups\",\"CRairline\",\"CRmessage\",\"CRdisasters\",\"CReconomic\",\"CRemotion\",\"CRwarm\",\"CRpolitical\",\"CRbias\",\"CRpltmsg\",\"CRprmemotion\",\"CRopn\",\"Newsag\",\"CRus\"]\n",
        "if TASK == \"mnli\" or TASK == \"mnli-mm\":\n",
        "  TASK_DIR = os.path.join(GLUE_DIR, \"MNLI\" )\n",
        "elif TASK == \"cola\": \n",
        "  TASK_DIR = os.path.join(GLUE_DIR, \"CoLA\")\n",
        "else:\n",
        "  TASK_DIR = os.path.join(GLUE_DIR, TASK.upper())\n",
        "\n",
        "num_labels = glue_tasks_num_labels[\"mnli\" if \"mnli\" in TASK else TASK]\n",
        "output_mode = glue_output_modes[TASK]\n",
        "processor = glue_processors[TASK]()\n",
        "print(num_labels)\n",
        "\n",
        "LEARNING_RATE =  3e-4 #@param {type:\"number\"}\n",
        "WARMUP_RATIO =   0.1 #@param {type:\"number\"}\n",
        "\n",
        "SAVED_MODELS_DIR = \"saved_models\"\n",
        "if not os.path.isdir(SAVED_MODELS_DIR):\n",
        "    os.mkdir(SAVED_MODELS_DIR)\n",
        "if not os.path.isdir(os.path.join(SAVED_MODELS_DIR, TASK)):\n",
        "    os.mkdir(os.path.join(SAVED_MODELS_DIR, TASK))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsulSyzAsXVs",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "fb522d77b2a242ebaa0d1c229301c2ca",
            "953ee1e67fc44ad08d9e4aac65be7f6e",
            "611b2e6776934b6f9a7a4171f39035a8",
            "a176f20f924b47fb8e47365b47ea9ba7",
            "797eb394179243589d819b2d901bd094",
            "e028cbdc8c2d46de9a7051df965bc357",
            "96abf4dd02ab4b219f06a185d1ed7b66",
            "7fbc90fbd3d942ab948ab7b827e93118",
            "6816bd47755648429147c899edd19f95",
            "be56d38b385048ee96c829da97a4d2a3",
            "379a41f18a9646ada57610f74f78342e",
            "e99bdefb95c0429d965884d6fd0fb70d",
            "66fafa9321f548c989669d9da8fc03ac",
            "a859baf1caea445590b72b17394d1738",
            "bc6bd261434e4e11822e9d4f9c94777f",
            "a562a936ed6b4d58aebfff7427885ce5",
            "87a283a318874f83b637a58cf9bd7d6b",
            "5535a4615219465abfdb39e5b330ebd3",
            "4f165300e51c44f9abe8dbac69de3b0e",
            "351daf480fcc4085bf60e5d9bb7cc5da",
            "152dab38d5b040a3a85476962bd85370",
            "e67405cc3fe948558dce8ccf155e2f59",
            "1b43c171934d49ec909d0dbfb87df994",
            "d4f2e10bbddc4f35b809a0a503ad5848"
          ]
        },
        "outputId": "4b5ebcd7-17fc-446d-d7a9-27b9626a2238"
      },
      "source": [
        "# @title Load Tokenizer, Model\n",
        "pretrained_weights = \"bert-base-uncased\" \n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_weights)\n",
        "model = AutoModelWithHeads.from_pretrained(pretrained_weights)\n",
        "model.add_classification_head(TASK, num_labels=num_labels, layers=2)\n",
        "\n",
        "model.add_adapter(adapter_name=TASK, adapter_type=AdapterType.text_task, config=\"houlsby\")\n",
        "model.train_adapter([TASK])\n",
        "model.set_active_adapters(TASK)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb522d77b2a242ebaa0d1c229301c2ca",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6816bd47755648429147c899edd19f95",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87a283a318874f83b637a58cf9bd7d6b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5XtJcWQkWHf",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title Example to Features\n",
        "def example_to_features(mode, tokenizer, max_len, label_list, output_mode):\n",
        "  if mode == \"val\":\n",
        "    examples = processor.get_dev_examples(TASK_DIR)\n",
        "  elif mode == \"test\":\n",
        "    examples = processor.get_test_examples(TASK_DIR)\n",
        "  elif mode == \"train\":\n",
        "    examples = processor.get_train_examples(TASK_DIR)\n",
        "\n",
        "  features = glue_convert_examples_to_features(\n",
        "                      examples,\n",
        "                      tokenizer,\n",
        "                      max_length=max_len,\n",
        "                      label_list=label_list,\n",
        "                      output_mode=output_mode,\n",
        "                  )\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6V0GFRh5HvJ",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# @title Load Datasets\n",
        "train_dataset = example_to_features(\"train\", tokenizer, MAX_LENGTH, processor.get_labels(), output_mode)\n",
        "val_dataset = example_to_features(\"val\", tokenizer, MAX_LENGTH, processor.get_labels(), output_mode)\n",
        "test_dataset = example_to_features(\"test\", tokenizer, MAX_LENGTH, processor.get_labels(), output_mode)\n",
        "\n",
        "data_collator = DefaultDataCollator()\n",
        "train_dataloader = DataLoader(train_dataset, \n",
        "                              batch_size = BATCH_SIZE,\n",
        "                              sampler = RandomSampler(train_dataset),\n",
        "                              collate_fn=data_collator.collate_batch)\n",
        "val_dataloader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size = BATCH_SIZE,\n",
        "            sampler = SequentialSampler(val_dataset),\n",
        "            collate_fn=data_collator.collate_batch)\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size = BATCH_SIZE,\n",
        "            sampler = SequentialSampler(test_dataset),\n",
        "            collate_fn=data_collator.collate_batch)\n",
        "\n",
        "total_steps = len(train_dataloader) * EPOCHS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-dhbj_8qICs",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title Compute Metrics\n",
        "def simple_accuracy(preds, labels):\n",
        "    return (preds == labels).mean()\n",
        "\n",
        "def f1(preds, labels):\n",
        "    return f1_score(y_true=labels, y_pred=preds)\n",
        "\n",
        "def spearman(preds, labels):\n",
        "    return spearmanr(preds, labels)[0]\n",
        "\n",
        "def pearson(preds, labels):\n",
        "    return pearsonr(preds, labels)[0]\n",
        "\n",
        "def get_metricName(task_name):\n",
        "    if task_name == \"mrpc\" or task_name == \"qqp\":\n",
        "        return \"f1\"\n",
        "    elif task_name == \"cola\":\n",
        "        return \"mcc\"\n",
        "    elif task_name == \"sts-b\":\n",
        "        return \"spearmanr\"\n",
        "    else:\n",
        "        return \"acc\"\n",
        "\n",
        "def glue_compute_metrics(metric_name, preds, labels):\n",
        "    if output_mode == \"classification\":\n",
        "        preds = np.argmax(preds, axis=1)\n",
        "    elif output_mode == \"regression\":\n",
        "        preds = np.squeeze(preds)\n",
        "\n",
        "    assert len(preds) == len(labels)\n",
        "\n",
        "    if metric_name == \"f1\":\n",
        "        return f1(preds, labels)\n",
        "    elif metric_name == \"mcc\":\n",
        "        return matthews_corrcoef(labels, preds)\n",
        "    elif metric_name == \"spearmanr\":\n",
        "        return spearman(preds, labels)\n",
        "    else:\n",
        "        return simple_accuracy(preds, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiVxzty5AS7W",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title Optimizer\n",
        "optimizer = AdamW(model.parameters(), \n",
        "                  lr=LEARNING_RATE, \n",
        "                  eps=1e-6)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                num_warmup_steps=int(WARMUP_RATIO * total_steps),\n",
        "                num_training_steps=total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll4_vTnhFYgb",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "bf5d1a91-1b5d-4ed9-cea6-d68e885444f9"
      },
      "source": [
        "# @title Train & Eval\n",
        "metric_name = get_metricName(TASK)\n",
        "history = {'train': {}, 'validation': {}}\n",
        "history['train']['loss'] = []\n",
        "history['validation']['loss'] = []\n",
        "history['validation']['val_' + metric_name] = []\n",
        "best_avg_val_loss = np.inf\n",
        "best_avg_val_score = 0\n",
        "\n",
        "model = model.to(device)\n",
        "model.zero_grad()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  # Training\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "  for inputs in train_dataloader:\n",
        "    for k, v in inputs.items():\n",
        "      inputs[k] = v.to(device)\n",
        "    \n",
        "    loss, logits = model(**inputs, adapter_names=[TASK])\n",
        "    train_loss += loss.item()\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    model.zero_grad()\n",
        "\n",
        "  avg_train_loss = train_loss / len(train_dataloader) \n",
        "  print(\"Epoch: {0}\".format(epoch+1))\n",
        "  print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "\n",
        "  # Evaluating\n",
        "  model.eval()\n",
        "  val_loss = 0\n",
        "  val_score = 0\n",
        "  for inputs in val_dataloader:\n",
        "    for k, v in inputs.items():\n",
        "      inputs[k] = v.to(device)\n",
        "    \n",
        "    with torch.no_grad(): \n",
        "      loss, logits = model(**inputs, adapter_names=[TASK])\n",
        "    val_loss += loss.item()\n",
        "    \n",
        "    preds = logits.detach().cpu().numpy()\n",
        "    label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
        "    val_score += glue_compute_metrics(metric_name, preds, label_ids)\n",
        "  \n",
        "  avg_val_loss = val_loss / len(val_dataloader) \n",
        "  avg_val_score = val_score / len(val_dataloader) \n",
        "  print(\"Average validation loss: {0:.2f}\".format(avg_val_loss))\n",
        "  print(\"Average validation {0}: {1:.3f}\".format(metric_name, avg_val_score))\n",
        "\n",
        "  # Logg and save\n",
        "  history['train']['loss'].append(avg_train_loss)\n",
        "  history['validation']['loss'].append(avg_val_loss)\n",
        "  history['validation']['val_' + metric_name].append(avg_val_score)\n",
        "  \n",
        "  if avg_val_score > best_avg_val_score or (avg_val_score == best_avg_val_score and avg_val_loss < best_avg_val_loss):\n",
        "    best_avg_val_score = avg_val_score\n",
        "    best_avg_val_loss = avg_val_loss\n",
        "    \n",
        "    os.mkdir(os.path.join(SAVED_MODELS_DIR, TASK, str(epoch+1)))\n",
        "    torch.save(model.state_dict(), os.path.join(SAVED_MODELS_DIR, TASK, str(epoch+1), 'saved_model.pt'))\n",
        "    print(\"Model saved as best model\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Average training loss: 0.70\n",
            "Average validation loss: 0.71\n",
            "Average validation acc: 0.569\n",
            "Model saved as best model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-983357de4494>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTASK\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxxjTsdgdE7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Restore the best model\n",
        "model = AutoModelWithHeads.from_pretrained(pretrained_weights)\n",
        "model.add_classification_head(TASK, num_labels=num_labels, layers=2)\n",
        "model.add_adapter(adapter_name=TASK, adapter_type=AdapterType.text_task, config=\"houlsby\")\n",
        "model.train_adapter([TASK])\n",
        "\n",
        "BEST_EPOCH = 0#@param {type:\"integer\"}\n",
        "model.load_state_dict(torch.load(os.path.join(SAVED_MODELS_DIR, TASK, str(BEST_EPOCH), \"saved_model.pt\"))\n",
        "model.set_active_adapters(TASK)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL8Yyvuib8z-",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# @title Test Additional Datasets\n",
        "test_score = 0\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "for inputs in tqdm(test_dataloader):\n",
        "  for k, v in inputs.items():\n",
        "    inputs[k] = v.to(device)\n",
        "  \n",
        "  with torch.no_grad(): \n",
        "    _, logits = model(**inputs, adapter_names=[TASK])\n",
        "    \n",
        "  test_preds = logits.detach().cpu().numpy()\n",
        "  label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
        "  test_score += glue_compute_metrics(metric_name, test_preds, label_ids)\n",
        "\n",
        "avg_test_score = test_score / len(test_dataloader) \n",
        "print(\"Average validation {0}: {1:.3f}\".format(metric_name, avg_test_score))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HimA3TqEFSa3",
        "colab_type": "text"
      },
      "source": [
        "# Save Model to Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhmcitQxcPmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNdvh8DQmuUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp0Rn_en6lYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/drive/My\\ Drive/Saved\\ Models/$TASK"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hcuFdDZk8FP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r saved_models/$TASK/?/* /content/drive/My\\ Drive/Saved\\ Models/$TASK/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4svLK7RuPx8g",
        "colab_type": "text"
      },
      "source": [
        "#Archived Codes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe5WB2CHP0YM",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "300fe828-d946-46f7-87e7-7a4f833781ec"
      },
      "source": [
        "# @title Trainable Params\n",
        "for w in model.named_parameters():\n",
        "  # if w[1].requires_grad == True:\n",
        "  print(w[0], end=\",\\t\")\n",
        "  print(w[1].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert.embeddings.word_embeddings.weight,\ttorch.Size([30522, 768])\n",
            "bert.embeddings.position_embeddings.weight,\ttorch.Size([512, 768])\n",
            "bert.embeddings.token_type_embeddings.weight,\ttorch.Size([2, 768])\n",
            "bert.embeddings.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.embeddings.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.0.attention.self.query.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.0.attention.self.query.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.0.attention.self.key.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.0.attention.self.key.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.0.attention.self.value.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.0.attention.self.value.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.0.attention.output.dense.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.0.attention.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.0.attention.output.attention_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.0.attention.output.attention_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.0.attention.output.attention_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.0.attention.output.attention_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.0.intermediate.dense.weight,\ttorch.Size([3072, 768])\n",
            "bert.encoder.layer.0.intermediate.dense.bias,\ttorch.Size([3072])\n",
            "bert.encoder.layer.0.output.dense.weight,\ttorch.Size([768, 3072])\n",
            "bert.encoder.layer.0.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.0.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.0.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.0.output.layer_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.0.output.layer_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.0.output.layer_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.0.output.layer_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.1.attention.self.query.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.1.attention.self.query.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.1.attention.self.key.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.1.attention.self.key.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.1.attention.self.value.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.1.attention.self.value.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.1.attention.output.dense.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.1.attention.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.1.attention.output.attention_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.1.attention.output.attention_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.1.attention.output.attention_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.1.attention.output.attention_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.1.intermediate.dense.weight,\ttorch.Size([3072, 768])\n",
            "bert.encoder.layer.1.intermediate.dense.bias,\ttorch.Size([3072])\n",
            "bert.encoder.layer.1.output.dense.weight,\ttorch.Size([768, 3072])\n",
            "bert.encoder.layer.1.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.1.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.1.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.1.output.layer_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.1.output.layer_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.1.output.layer_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.1.output.layer_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.2.attention.self.query.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.2.attention.self.query.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.2.attention.self.key.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.2.attention.self.key.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.2.attention.self.value.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.2.attention.self.value.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.2.attention.output.dense.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.2.attention.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.2.attention.output.attention_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.2.attention.output.attention_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.2.attention.output.attention_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.2.attention.output.attention_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.2.intermediate.dense.weight,\ttorch.Size([3072, 768])\n",
            "bert.encoder.layer.2.intermediate.dense.bias,\ttorch.Size([3072])\n",
            "bert.encoder.layer.2.output.dense.weight,\ttorch.Size([768, 3072])\n",
            "bert.encoder.layer.2.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.2.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.2.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.2.output.layer_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.2.output.layer_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.2.output.layer_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.2.output.layer_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.3.attention.self.query.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.3.attention.self.query.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.3.attention.self.key.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.3.attention.self.key.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.3.attention.self.value.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.3.attention.self.value.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.3.attention.output.dense.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.3.attention.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.3.attention.output.attention_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.3.attention.output.attention_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.3.attention.output.attention_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.3.attention.output.attention_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.3.intermediate.dense.weight,\ttorch.Size([3072, 768])\n",
            "bert.encoder.layer.3.intermediate.dense.bias,\ttorch.Size([3072])\n",
            "bert.encoder.layer.3.output.dense.weight,\ttorch.Size([768, 3072])\n",
            "bert.encoder.layer.3.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.3.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.3.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.3.output.layer_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.3.output.layer_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.3.output.layer_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.3.output.layer_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.4.attention.self.query.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.4.attention.self.query.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.4.attention.self.key.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.4.attention.self.key.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.4.attention.self.value.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.4.attention.self.value.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.4.attention.output.dense.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.4.attention.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.4.attention.output.attention_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.4.attention.output.attention_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.4.attention.output.attention_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.4.attention.output.attention_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.4.intermediate.dense.weight,\ttorch.Size([3072, 768])\n",
            "bert.encoder.layer.4.intermediate.dense.bias,\ttorch.Size([3072])\n",
            "bert.encoder.layer.4.output.dense.weight,\ttorch.Size([768, 3072])\n",
            "bert.encoder.layer.4.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.4.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.4.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.4.output.layer_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.4.output.layer_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.4.output.layer_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.4.output.layer_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.5.attention.self.query.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.5.attention.self.query.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.5.attention.self.key.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.5.attention.self.key.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.5.attention.self.value.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.5.attention.self.value.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.5.attention.output.dense.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.5.attention.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.5.attention.output.attention_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.5.attention.output.attention_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.5.attention.output.attention_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.5.attention.output.attention_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.5.intermediate.dense.weight,\ttorch.Size([3072, 768])\n",
            "bert.encoder.layer.5.intermediate.dense.bias,\ttorch.Size([3072])\n",
            "bert.encoder.layer.5.output.dense.weight,\ttorch.Size([768, 3072])\n",
            "bert.encoder.layer.5.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.5.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.5.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.5.output.layer_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.5.output.layer_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.5.output.layer_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.5.output.layer_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.6.attention.self.query.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.6.attention.self.query.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.6.attention.self.key.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.6.attention.self.key.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.6.attention.self.value.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.6.attention.self.value.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.6.attention.output.dense.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.6.attention.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.6.attention.output.attention_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.6.attention.output.attention_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.6.attention.output.attention_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.6.attention.output.attention_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.6.intermediate.dense.weight,\ttorch.Size([3072, 768])\n",
            "bert.encoder.layer.6.intermediate.dense.bias,\ttorch.Size([3072])\n",
            "bert.encoder.layer.6.output.dense.weight,\ttorch.Size([768, 3072])\n",
            "bert.encoder.layer.6.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.6.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.6.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.6.output.layer_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.6.output.layer_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.6.output.layer_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.6.output.layer_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.7.attention.self.query.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.7.attention.self.query.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.7.attention.self.key.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.7.attention.self.key.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.7.attention.self.value.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.7.attention.self.value.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.7.attention.output.dense.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.7.attention.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.7.attention.output.attention_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.7.attention.output.attention_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.7.attention.output.attention_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.7.attention.output.attention_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.7.intermediate.dense.weight,\ttorch.Size([3072, 768])\n",
            "bert.encoder.layer.7.intermediate.dense.bias,\ttorch.Size([3072])\n",
            "bert.encoder.layer.7.output.dense.weight,\ttorch.Size([768, 3072])\n",
            "bert.encoder.layer.7.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.7.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.7.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.7.output.layer_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.7.output.layer_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.7.output.layer_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.7.output.layer_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.8.attention.self.query.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.8.attention.self.query.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.8.attention.self.key.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.8.attention.self.key.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.8.attention.self.value.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.8.attention.self.value.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.8.attention.output.dense.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.8.attention.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.8.attention.output.attention_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.8.attention.output.attention_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.8.attention.output.attention_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.8.attention.output.attention_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.8.intermediate.dense.weight,\ttorch.Size([3072, 768])\n",
            "bert.encoder.layer.8.intermediate.dense.bias,\ttorch.Size([3072])\n",
            "bert.encoder.layer.8.output.dense.weight,\ttorch.Size([768, 3072])\n",
            "bert.encoder.layer.8.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.8.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.8.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.8.output.layer_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.8.output.layer_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.8.output.layer_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.8.output.layer_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.9.attention.self.query.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.9.attention.self.query.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.9.attention.self.key.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.9.attention.self.key.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.9.attention.self.value.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.9.attention.self.value.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.9.attention.output.dense.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.9.attention.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.9.attention.output.attention_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.9.attention.output.attention_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.9.attention.output.attention_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.9.attention.output.attention_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.9.intermediate.dense.weight,\ttorch.Size([3072, 768])\n",
            "bert.encoder.layer.9.intermediate.dense.bias,\ttorch.Size([3072])\n",
            "bert.encoder.layer.9.output.dense.weight,\ttorch.Size([768, 3072])\n",
            "bert.encoder.layer.9.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.9.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.9.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.9.output.layer_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.9.output.layer_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.9.output.layer_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.9.output.layer_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.10.attention.self.query.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.10.attention.self.query.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.10.attention.self.key.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.10.attention.self.key.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.10.attention.self.value.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.10.attention.self.value.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.10.attention.output.dense.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.10.attention.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.10.attention.output.attention_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.10.attention.output.attention_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.10.attention.output.attention_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.10.attention.output.attention_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.10.intermediate.dense.weight,\ttorch.Size([3072, 768])\n",
            "bert.encoder.layer.10.intermediate.dense.bias,\ttorch.Size([3072])\n",
            "bert.encoder.layer.10.output.dense.weight,\ttorch.Size([768, 3072])\n",
            "bert.encoder.layer.10.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.10.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.10.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.10.output.layer_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.10.output.layer_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.10.output.layer_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.10.output.layer_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.11.attention.self.query.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.11.attention.self.query.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.11.attention.self.key.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.11.attention.self.key.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.11.attention.self.value.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.11.attention.self.value.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.11.attention.output.dense.weight,\ttorch.Size([768, 768])\n",
            "bert.encoder.layer.11.attention.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.11.attention.output.attention_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.11.attention.output.attention_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.11.attention.output.attention_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.11.attention.output.attention_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.11.intermediate.dense.weight,\ttorch.Size([3072, 768])\n",
            "bert.encoder.layer.11.intermediate.dense.bias,\ttorch.Size([3072])\n",
            "bert.encoder.layer.11.output.dense.weight,\ttorch.Size([768, 3072])\n",
            "bert.encoder.layer.11.output.dense.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.11.output.LayerNorm.weight,\ttorch.Size([768])\n",
            "bert.encoder.layer.11.output.LayerNorm.bias,\ttorch.Size([768])\n",
            "bert.encoder.layer.11.output.layer_text_task_adapters.rte.adapter_down.0.weight,\ttorch.Size([64, 768])\n",
            "bert.encoder.layer.11.output.layer_text_task_adapters.rte.adapter_down.0.bias,\ttorch.Size([64])\n",
            "bert.encoder.layer.11.output.layer_text_task_adapters.rte.adapter_up.weight,\ttorch.Size([768, 64])\n",
            "bert.encoder.layer.11.output.layer_text_task_adapters.rte.adapter_up.bias,\ttorch.Size([768])\n",
            "bert.pooler.dense.weight,\ttorch.Size([768, 768])\n",
            "bert.pooler.dense.bias,\ttorch.Size([768])\n",
            "heads.rte.1.weight,\ttorch.Size([768, 768])\n",
            "heads.rte.1.bias,\ttorch.Size([768])\n",
            "heads.rte.4.weight,\ttorch.Size([2, 768])\n",
            "heads.rte.4.bias,\ttorch.Size([2])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}